{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "<p>The collection and use of data for football analysis is an emerging and rapidly developing sector. Describing aspects of an highly random sport like football through data is certainly an ambitious goal, but more and more teams and companies are investing in this field, recognising its enormous potential. Unfortunately, the availability of free open data providing modern performance metrics is still very limited. So, after a careful research, we have selected the following resources in order to build a dataset containing the most important performance metrics and more: Understat, Api-football and Fbref.\n",
        "\n",
        "We decide not to establish at first a structure for our dataset, but we will explore the available data and then assess which structure is the most appropriate.\n",
        "\n",
        "Below is the code written to carry out the data acquisition operations and some data cleaning operations in the download phase. We dedicate a section to each data resource.</p>"
      ],
      "metadata": {
        "id": "kyKnq9GpIy6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understat\n",
        "\n",
        "<p>Understat is a website that provides statistics of teams and players from Premier League (England and Wales), Serie A (Italy), La Liga (Spain), Bundesliga (Germany), Ligue 1 (France) and Russian Premier League (Russia), starting from the 2014/2015 season to the actual season. We consider the seasons from 2014/2015 to 2020/2021 of the top 5 football leagues (Premier League, Serie A, La Liga, Bundesliga and Ligue 1).\n",
        "We chose Understat as a data source because it provides modern and specific performance statistics in football. In order to obtain the data, a special Python package is made available. In particular, to download the data we use the function get_league_players. This function receives an integer indicating the season and a string indicating the league name. It returns a list of dictionaries, one dictionary for each player containing team, position, games played and various game statistics for each player are provided.</p>\n",
        "\n",
        "<p>We discard players who have played less than 15 games in each season, because we consider a season with less than 15 games played non-evaluable. For the moment we decide to download all the data provided, we only remove the field containing the player's Understat id. We proceed with some simple data cleaning and normalisation operations directly in the download phase. Some numeric data are supplied with values in a string format. We identify the fields with this problem and convert the value types accordingly. Players who changed teams throughout the season, while remaining in the same league, present a double value for the team field and aggregate statistics. We decide to keep only the statistics of the first of the two teams. We consider this to be an acceptable approximation, as for the application that this dataset was conceived the team is of little relevance, while the league is far more important. In order to have an indication of the impact of this approximation, we obtain the percentage of players that present this characteristic. They are 0.03% of the total number of players.<p/>\n",
        "\n",
        "<p>We generate a json file for each season of a given league, resulting in 35 json files (7 seasons, 5 leagues). We select the options encoding='utf-8' and ensure_ascii=False in order to handle special characters correctly. We insert a specific provision for the apostrophe character while downloading.</p>\n",
        "\n",
        "<p>Since we want to download data of players from several sources, we need to find a field that will allow us to aggregate the different datasets. We think that the most suitable field is the name of the players, then we save in a separate structure the names of all considered players. We want to get a list of lists, a sub list will contain only the players for each given season and league. In addition to the names we also save the teams they belong to, because it might be useful to solve ambiguities. </p>\n",
        "\n",
        "<p>Then after we obtained the list containing the names of the players, we add that list to another list called football_players. So once we get a list of lists, where each sublist contains the names of players from a certain league in a certain season, we proceed in the same way with the players' teams, obtaining also in this case a list of lists called players_teams. The usefulness and practical application of this procedure will become clearer later on.</p>\n"
      ],
      "metadata": {
        "id": "q7VHk8XaN7dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install understat"
      ],
      "metadata": {
        "id": "2h8a9gaZgWlL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9203fe7e-652a-44d1-a751-91d5acbcdaba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting understat\n",
            "  Downloading understat-0.1.4-py3-none-any.whl (7.3 kB)\n",
            "Collecting codecov\n",
            "  Downloading codecov-2.1.12-py2.py3-none-any.whl (16 kB)\n",
            "Collecting pytest-aiohttp\n",
            "  Downloading pytest_aiohttp-1.0.3-py3-none-any.whl (8.5 kB)\n",
            "Collecting pytest-cov\n",
            "  Downloading pytest_cov-3.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from understat) (3.6.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from understat) (4.6.3)\n",
            "Collecting pytest-mock\n",
            "  Downloading pytest_mock-3.6.1-py3-none-any.whl (12 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 48.0 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 54.6 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->understat) (2.0.10)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 47.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->understat) (3.10.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->understat) (21.4.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->understat) (2.10)\n",
            "Requirement already satisfied: requests>=2.7.9 in /usr/local/lib/python3.7/dist-packages (from codecov->understat) (2.23.0)\n",
            "Requirement already satisfied: coverage in /usr/local/lib/python3.7/dist-packages (from codecov->understat) (3.7.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->understat) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->understat) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov->understat) (2021.10.8)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->understat) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->understat) (1.15.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->understat) (1.11.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->understat) (8.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->understat) (57.4.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->understat) (1.4.0)\n",
            "Collecting pytest-asyncio>=0.17.2\n",
            "  Downloading pytest_asyncio-0.17.2-py3-none-any.whl (16 kB)\n",
            "Collecting pytest\n",
            "  Downloading pytest-6.2.5-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 76.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest->understat) (1.1.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pytest->understat) (0.10.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest->understat) (4.10.0)\n",
            "  Downloading pytest-6.2.4-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 68.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytest->understat) (21.3)\n",
            "  Downloading pytest-6.2.3-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 45.6 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.2-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 69.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.1-py3-none-any.whl (279 kB)\n",
            "\u001b[K     |████████████████████████████████| 279 kB 62.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.2.0-py3-none-any.whl (279 kB)\n",
            "\u001b[K     |████████████████████████████████| 279 kB 56.6 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.1.2-py3-none-any.whl (272 kB)\n",
            "\u001b[K     |████████████████████████████████| 272 kB 57.2 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.1.1-py3-none-any.whl (272 kB)\n",
            "\u001b[K     |████████████████████████████████| 272 kB 59.3 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.1.0-py3-none-any.whl (272 kB)\n",
            "\u001b[K     |████████████████████████████████| 272 kB 45.0 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of pytest-aiohttp to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pytest-aiohttp\n",
            "  Downloading pytest_aiohttp-1.0.2-py3-none-any.whl (8.5 kB)\n",
            "  Downloading pytest_aiohttp-1.0.1-py3-none-any.whl (8.3 kB)\n",
            "  Downloading pytest_aiohttp-1.0.0-py3-none-any.whl (8.5 kB)\n",
            "  Downloading pytest_aiohttp-0.3.0-py3-none-any.whl (3.9 kB)\n",
            "Collecting coverage[toml]>=5.2.1\n",
            "  Downloading coverage-6.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 73.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.7/dist-packages (from coverage[toml]>=5.2.1->pytest-cov->understat) (2.0.0)\n",
            "Collecting pytest\n",
            "  Downloading pytest-6.0.2-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 73.5 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.0.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 28.5 MB/s \n",
            "\u001b[?25h  Downloading pytest-6.0.0-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 72.5 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.4.3-py3-none-any.whl (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 70.0 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.4.2-py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 75.3 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.4.1-py3-none-any.whl (246 kB)\n",
            "\u001b[K     |████████████████████████████████| 246 kB 61.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.4.0-py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 55.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.5-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 55.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 59.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.3-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 64.6 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.2-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 62.1 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.1-py3-none-any.whl (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 61.5 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.3.0-py3-none-any.whl (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 54.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.2.4-py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 50.3 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.2.3-py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 48.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.2.2-py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 61.2 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.2.1-py3-none-any.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 54.1 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.2.0-py3-none-any.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 59.1 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[K     |████████████████████████████████| 224 kB 61.3 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.1.2-py3-none-any.whl (224 kB)\n",
            "\u001b[K     |████████████████████████████████| 224 kB 57.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.1.1-py3-none-any.whl (223 kB)\n",
            "\u001b[K     |████████████████████████████████| 223 kB 60.0 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.1.0-py3-none-any.whl (223 kB)\n",
            "\u001b[K     |████████████████████████████████| 223 kB 61.9 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.0.1-py3-none-any.whl (221 kB)\n",
            "\u001b[K     |████████████████████████████████| 221 kB 48.0 MB/s \n",
            "\u001b[?25h  Downloading pytest-5.0.0-py3-none-any.whl (221 kB)\n",
            "\u001b[K     |████████████████████████████████| 221 kB 46.7 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.11-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 57.0 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.10-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 49.5 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.9-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 56.6 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.8-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 59.5 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.7-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 38.1 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.6-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 61.6 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.5-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 57.6 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.4-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 59.2 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.3-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 62.1 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.2-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 60.4 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.1-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 52.8 MB/s \n",
            "\u001b[?25h  Downloading pytest-4.6.0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 60.5 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of coverage to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of coverage[toml] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting coverage[toml]>=5.2.1\n",
            "  Downloading coverage-6.1.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 61.6 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.1.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 79.3 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 53.2 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.0.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (253 kB)\n",
            "\u001b[K     |████████████████████████████████| 253 kB 53.9 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (252 kB)\n",
            "\u001b[K     |████████████████████████████████| 252 kB 41.3 MB/s \n",
            "\u001b[?25h  Downloading coverage-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (252 kB)\n",
            "\u001b[K     |████████████████████████████████| 252 kB 53.9 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.5-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n",
            "\u001b[K     |████████████████████████████████| 242 kB 54.8 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of coverage to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of coverage[toml] to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading coverage-5.4-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n",
            "\u001b[K     |████████████████████████████████| 242 kB 46.0 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.3.1-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n",
            "\u001b[K     |████████████████████████████████| 242 kB 42.7 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.3-cp37-cp37m-manylinux1_x86_64.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 67.3 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.2.1-cp37-cp37m-manylinux1_x86_64.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 60.8 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of pytest-cov to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pytest-cov\n",
            "  Downloading pytest_cov-2.12.1-py2.py3-none-any.whl (20 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading pytest_cov-2.12.0-py2.py3-none-any.whl (20 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading pytest_cov-2.11.1-py2.py3-none-any.whl (20 kB)\n",
            "  Downloading pytest_cov-2.11.0-py2.py3-none-any.whl (20 kB)\n",
            "  Downloading pytest_cov-2.10.1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting coverage\n",
            "  Downloading coverage-5.2-cp37-cp37m-manylinux1_x86_64.whl (229 kB)\n",
            "\u001b[K     |████████████████████████████████| 229 kB 44.6 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.1-cp37-cp37m-manylinux1_x86_64.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 61.5 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.0.4-cp37-cp37m-manylinux1_x86_64.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 62.7 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.0.3-cp37-cp37m-manylinux1_x86_64.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 58.2 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.0.2-cp37-cp37m-manylinux1_x86_64.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 63.1 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.0.1-cp37-cp37m-manylinux1_x86_64.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 58.4 MB/s \n",
            "\u001b[?25h  Downloading coverage-5.0-cp37-cp37m-manylinux1_x86_64.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 54.5 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.5.4-cp37-cp37m-manylinux1_x86_64.whl (205 kB)\n",
            "\u001b[K     |████████████████████████████████| 205 kB 55.3 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.5.3-cp37-cp37m-manylinux1_x86_64.whl (204 kB)\n",
            "\u001b[K     |████████████████████████████████| 204 kB 51.7 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.5.2-cp37-cp37m-manylinux1_x86_64.whl (205 kB)\n",
            "\u001b[K     |████████████████████████████████| 205 kB 52.7 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.5.1-cp37-cp37m-manylinux1_x86_64.whl (202 kB)\n",
            "\u001b[K     |████████████████████████████████| 202 kB 47.2 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.5.tar.gz (378 kB)\n",
            "\u001b[K     |████████████████████████████████| 378 kB 52.2 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.4.2.tar.gz (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 55.9 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.4.1.tar.gz (369 kB)\n",
            "\u001b[K     |████████████████████████████████| 369 kB 48.4 MB/s \n",
            "\u001b[?25h  Downloading coverage-4.4.tar.gz (369 kB)\n",
            "\u001b[K     |████████████████████████████████| 369 kB 57.5 MB/s \n",
            "\u001b[?25hCollecting pytest-cov\n",
            "  Downloading pytest_cov-2.10.0-py2.py3-none-any.whl (19 kB)\n",
            "  Downloading pytest_cov-2.9.0-py2.py3-none-any.whl (19 kB)\n",
            "INFO: pip is looking at multiple versions of pytest-mock to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pytest-mock\n",
            "  Downloading pytest_mock-3.6.0-py3-none-any.whl (12 kB)\n",
            "  Downloading pytest_mock-3.5.1-py3-none-any.whl (12 kB)\n",
            "  Downloading pytest_mock-3.5.0-py3-none-any.whl (12 kB)\n",
            "  Downloading pytest_mock-3.4.0-py3-none-any.whl (11 kB)\n",
            "  Downloading pytest_mock-3.3.1-py3-none-any.whl (11 kB)\n",
            "  Downloading pytest_mock-3.3.0-py3-none-any.whl (11 kB)\n",
            "  Downloading pytest_mock-3.2.0-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, coverage, aiohttp, pytest-mock, pytest-cov, pytest-aiohttp, codecov, understat\n",
            "  Attempting uninstall: coverage\n",
            "    Found existing installation: coverage 3.7.1\n",
            "    Uninstalling coverage-3.7.1:\n",
            "      Successfully uninstalled coverage-3.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires coverage==3.7.1, but you have coverage 6.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "coveralls 0.5 requires coverage<3.999,>=3.6, but you have coverage 6.2 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 codecov-2.1.12 coverage-6.2 frozenlist-1.3.0 multidict-6.0.1 pytest-aiohttp-0.3.0 pytest-cov-2.9.0 pytest-mock-3.2.0 understat-0.1.4 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "import json\n",
        "import understat\n",
        "import pandas as pd\n",
        "from understat import Understat\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "from difflib import SequenceMatcher"
      ],
      "metadata": {
        "id": "_SEelAOagfwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHBvLEDmHBLo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b7bcb6a-8b95-4b49-9970-8618641012a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.030521969920377468\n"
          ]
        }
      ],
      "source": [
        "#the leagues and seasons considered\n",
        "leagues = ['epl', 'Serie A', 'La Liga', 'Bundesliga', 'Ligue 1']\n",
        "seasons = [2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
        "\n",
        "#the numeric fields\n",
        "numeric_key_list = ['id', 'games', 'time', 'goals', 'xG', 'assists', 'xA', 'shots', 'key_passes',\n",
        " 'yellow_cards', 'red_cards', 'npg', 'npxG', 'xGChain', 'xGBuildup']\n",
        "\n",
        "#list where we are going to add the player names\n",
        "football_players = []\n",
        "#the list where we are going to add the lists with player teams\n",
        "players_teams = []\n",
        "\n",
        "#To verify how much players contains two values for the field team_title\n",
        "players_same_team = []\n",
        "players_total = []\n",
        "\n",
        "for season in seasons:\n",
        "  for league in leagues:\n",
        "\n",
        "    #the list where we are going to add the dictionaires of the players\n",
        "    res=[]\n",
        "    #the list where we are going to add the players of a given season and league\n",
        "    players_seas_league = []\n",
        "    #the list where we are going to add the teams of a given season and league\n",
        "    players_teams_league = []\n",
        "\n",
        "    async def main():\n",
        "        async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False)) as session:\n",
        "            understat = Understat(session)\n",
        "            players = await understat.get_league_players(\n",
        "                league_name=league,\n",
        "                season=season\n",
        "            )\n",
        "\n",
        "            for i in range(len(players)):\n",
        "              #numeric statistics should be numeric\n",
        "              for key in numeric_key_list:\n",
        "                players[i][key] = float(players[i][key])\n",
        "                players[i][key] = round(players[i][key], 2)\n",
        "\n",
        "              if(players[i]['games'] >= 15):\n",
        "                players_total.append('1')\n",
        "                #players with double value for team field\n",
        "                if players[i]['team_title'].find(',') != -1:\n",
        "                  teams = players[i]['team_title'].split(',')\n",
        "                  players[i]['team_title'] = teams[0]\n",
        "                  players_same_team.append('1')\n",
        "\n",
        "                players[i].pop('id')\n",
        "\n",
        "                if players[i]['player_name'].find('\\'') != -1:\n",
        "                  players[i]['player_name'] = players[i]['player_name'].replace('&#039;', '\\'')\n",
        "\n",
        "                #we add name of players to the players_seas_league list\n",
        "                players_seas_league.append(players[i]['player_name'])\n",
        "                #we add the team of players to the players_teams_leagues list\n",
        "                players_teams_league.append(players[i]['team_title'])\n",
        "\n",
        "                #we add the dictionaire containing the data of players to the res list\n",
        "                element = dict(players[i])\n",
        "                res.append(element)\n",
        "              else:\n",
        "                continue\n",
        "\n",
        "            #After analyzing each players of a given league for a given season, we add the res list to a json file\n",
        "            # with open('stats_' + league + str(season) + '_players.json', 'a', encoding='utf-8') as outfile:\n",
        "            #   json.dump(res, outfile, ensure_ascii=False)\n",
        "\n",
        "            #we add the built sub list to the football_players and players_teams list\n",
        "            football_players.append(players_seas_league)\n",
        "            players_teams.append(players_teams_league)\n",
        "\n",
        "    loop = asyncio.get_event_loop()\n",
        "    loop.run_until_complete(main())\n",
        "\n",
        "#we print the percentage of players with double value in the team_title field\n",
        "print(len(players_same_team)/len(players_total))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Api-football\n",
        "<p>Api-football is a site that provides an api, called api-football-beta, which allows you to obtain data on players, teams, coaches, transfers and betting odds for various leagues around the world. The number of seasons available depends on the league. The game statistics are much simpler than the ones provided by Understat, so we decide to use this api only to obtain players' age, nationality, weight and height. Here again, we consider the seasons from 2014/2015 to 2020/2021 of the Premier League, Serie A, La Liga, Bundesliga and Ligue 1.</p>\n",
        "\n",
        "<p>We provide 2 strings as input parameters to the api, the first one indicating the season and the other one indicating the league id. The data available for a certain league and season are provided on several pages, so in the download stage we add a third string indicating the page. With an initial request we get the total number of pages available for a certain league and season and visit them all. The return value is a dictionary of dictionaries and lists. The fields we are interested in are contained in the player dictionary within the response list. We obtain a dictionary for each player.</p>\n",
        "\n",
        "<p>Again, we only consider players with more than 15 games in the season and implement some data cleaning operations directly in the download phase. Weight and height have string values, so we convert them into numerical values. The height is given in cm, the weight in kg. The age field indicates the current age of the player, however we are interested in the age of the player in a given season. We obtain this value from the birth date field. Moreover, we delete the fields of the player dictionary which are not of interest to us. </p>\n",
        "\n",
        "<p>As before, we generate a json file for each season of a given league, resulting in 35 json files (7 seasons, 5 leagues). We select the options encoding='utf-8' and ensure_ascii=False in order to treat special characters correctly.\n",
        "</p>\n",
        "\n",
        "<p>This data will need to be aggregated with the data downloaded from Understat. In order to do this, we need to make sure that each players have a field with the same value in both datasets, so that we can use that field as a key for aggregation. The field that seems most suitable for this operation is the name of the players. For each player, when downloading the data from api-football, we assign to the name field a name taken from the football_players list containing the names of the players downloaded from Understat. In this way the names are the same in both datasets. We explain below how we correctly associate the names of the players between the two datasets.</p>\n",
        "\n",
        "<p>We write a checkname function which, given the name of a player downloaded from api-football, determines the name of the corresponding player from the data downloaded from Understat. The implementation of this function is complex since the same player may be represented with syntactically different names in the two sites. The function we have implemented receives:\n",
        "\n",
        " * the players_teams sublist of the appropriate season and year (downloaded from Understat)\n",
        " * the football_players sublist of the appropriate season and year (downloaded from Understat),\n",
        " * the full name of the player (downloaded from api-football)\n",
        " * the common name (downloaded from api-football, name field)\n",
        " * the team (downloaded from api-football, team field)\n",
        "\n",
        "This function analyzes each string in football_players and calculates its similarity (using the SequenceMatcher criterion) with the full name and with the common name. The SequenceMatcher criterion is an implementation of the LCS distance that does not consider spaces and punctuation. We choose this distance, as opposed to e.g. edit distance, because we believe that differences in names are syntactic and not due to types. A value of 0.2 is added to the similarity value between the players names if the similarity between the team names overcome a predefined threshold (if the name and the team are similar, it is very likely that the player has been correctly recognised and we do not want to risk losing the association). If the maximum similarity is above a predefined threshold (0.8) then the corresponding string is returned, otherwise an empty string is returned and the corresponding player will not be included in the constructed dataset. The values of the thresholds were chosen after some testing. The commented code of the function is given below.</p>\n",
        "\n",
        "<p>We decided to implement this process directly in the download phase in order to facilitate the subsequent data aggregation process.</p>\n",
        "\n",
        "<p>We bring a key for the api as an example. The free subscription plan only allows 100 requests per day, so running the code at some point will lead to an error because more than 100 requests are needed to download all the data.</p>"
      ],
      "metadata": {
        "id": "ZIdeUDq2iEex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def checkname(players, name, nickname, squadre_understat, squadra_api):\n",
        "  maximum = 0\n",
        "  maximum_2 = 0\n",
        "\n",
        "  res = ''\n",
        "  res_2 = ''\n",
        "\n",
        "  for i in range(len(players)):\n",
        "    #if the selected players of the Understat list contains name and surname the comparison will be done with the full name\n",
        "    if(len(players[i].split(' ')) > 1):\n",
        "      if(maximum < SequenceMatcher(None, players[i], name).ratio()):\n",
        "        plus = 0\n",
        "        #if in addition to the name the similarity is also in the name of the team a plus will be added\n",
        "        if(SequenceMatcher(None, squadre_understat[i], squadra_api).ratio() > 0.7):\n",
        "          plus = plus + 0.2\n",
        "        maximum = SequenceMatcher(None, players[i], name).ratio() + plus\n",
        "        res = players[i]\n",
        "    #.. otherwise the comparison will be done with the common name, important trick for players like 'Isco' or 'Koke'\n",
        "    else:\n",
        "      nick =  SequenceMatcher(None, players[i], nickname).ratio()\n",
        "      if(maximum_2 < nick ):\n",
        "        plus = 0\n",
        "        if (SequenceMatcher(None, squadre_understat[i], squadra_api).ratio() > 0.7):\n",
        "          plus = plus + 0.2\n",
        "        maximum_2 =  nick + plus\n",
        "        res_2 = players[i]\n",
        "\n",
        "  if(maximum > 0.8): #priority to a possible similarity with the full name\n",
        "    return res\n",
        "  else:\n",
        "    if(maximum_2 > 0.8):\n",
        "      return res_2\n",
        "    else:\n",
        "      return ''"
      ],
      "metadata": {
        "id": "3clrJ-y384Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "leagues_ids = [\"39\", \"135\", \"140\", \"78\", \"61\"]\n",
        "seasons = [\"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]\n",
        "#iteration index that we are going to use to give the football_players and players_teams sublist\n",
        "iteration = 0\n",
        "\n",
        "key = ''\n",
        "\n",
        "for season in seasons:\n",
        "  for league_id, league in zip(leagues_ids, leagues):\n",
        "\n",
        "    #list where we are going to add the downloaded dictionaires\n",
        "    res = []\n",
        "\n",
        "    #we obtain the number of pages for a given season and league\n",
        "    url = \"https://api-football-beta.p.rapidapi.com/players\"\n",
        "    querystring = {\"season\": season , \"league\": league_id}\n",
        "    headers = {\n",
        "        'x-rapidapi-host': \"api-football-beta.p.rapidapi.com\",\n",
        "        'x-rapidapi-key':  key\n",
        "        }\n",
        "    result = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
        "    diz=result.json()\n",
        "    pages = diz['paging']['total']\n",
        "\n",
        "    for page in range(1, pages + 1):\n",
        "      page = str(page)\n",
        "\n",
        "      url = \"https://api-football-beta.p.rapidapi.com/players\"\n",
        "      querystring = {\"season\":season,\"league\":league_id, \"page\":page}\n",
        "      headers = {\n",
        "          'x-rapidapi-host': \"api-football-beta.p.rapidapi.com\",\n",
        "          'x-rapidapi-key':  key\n",
        "          }\n",
        "      response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
        "      aDict = response.json()\n",
        "      jsonStr = json.dumps(aDict, ensure_ascii=False)\n",
        "      diz = json.loads(jsonStr, encoding='utf-8')\n",
        "\n",
        "      for j in range(len(diz['response'])):\n",
        "          if diz['response'][j]['statistics'][0]['games']['appearences'] is not None:\n",
        "            if(diz['response'][j]['statistics'][0]['games']['appearences'] >= 15):\n",
        "\n",
        "              #we obtain the first name and last name\n",
        "              first_name = diz['response'][j]['player']['firstname'].split(' ')\n",
        "              second_name = diz['response'][j]['player']['lastname'].split(' ')\n",
        "              name = first_name[0]\n",
        "              #trick to deal surname like De Bruyne or Di Maria\n",
        "              if(len(second_name[0]) > 2):\n",
        "                surname = second_name[0]\n",
        "              else:\n",
        "                if(len(second_name) > 1):\n",
        "                  surname = second_name[0] + second_name[1]\n",
        "                else:\n",
        "                  surname = second_name[0]\n",
        "\n",
        "              player_name = name + ' ' + surname\n",
        "              #we assaign the return value of checkname to the name field\n",
        "              diz['response'][j]['player']['name'] = checkname(football_players[iteration], player_name,\n",
        "                                                               diz['response'][j]['player']['name'],\n",
        "                                                               players_teams[iteration], diz['response'][j]['statistics'][0]['team']['name'])\n",
        "\n",
        "              if(diz['response'][j]['player']['name'] != ''):\n",
        "                #we obtain the age of the players in each given season\n",
        "                year_birth = diz['response'][j]['player']['birth']['date'][:4]\n",
        "                diz['response'][j]['player']['age'] = int(season) - int(year_birth)\n",
        "\n",
        "                #we remove the fields that we are not interested in\n",
        "                diz['response'][j].pop('statistics')\n",
        "                diz['response'][j]['player'].pop('birth')\n",
        "                diz['response'][j]['player'].pop('firstname')\n",
        "                diz['response'][j]['player'].pop('lastname')\n",
        "                diz['response'][j]['player'].pop('injured')\n",
        "                diz['response'][j]['player'].pop('id')\n",
        "\n",
        "                #we transform height and weight in numeric values\n",
        "                if isinstance(diz['response'][j]['player']['height'], str):\n",
        "                  diz['response'][j]['player']['height'] = diz['response'][j]['player']['height'][:-2]\n",
        "                  diz['response'][j]['player']['height'] = int(diz['response'][j]['player']['height'])\n",
        "                if isinstance(diz['response'][j]['player']['weight'], str):\n",
        "                  diz['response'][j]['player']['weight'] = diz['response'][j]['player']['weight'][:-2]\n",
        "                  diz['response'][j]['player']['weight'] = int(diz['response'][j]['player']['weight'])\n",
        "\n",
        "                #we add the dictionaires of the players to the res list\n",
        "                element = dict(diz['response'][j]['player'])\n",
        "                res.append(element)\n",
        "              else:\n",
        "                continue\n",
        "    iteration = iteration + 1\n",
        "    #analyzed each players of a league for a given season, we add the res list in a json file\n",
        "    with open(season + league + '_players.json', 'a', encoding='utf-8') as outfile:\n",
        "        json.dump(res, outfile, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "oa0LOiR2lpJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fbref\n",
        "\n",
        "<p> Fbref is a site that provides a large amount of very modern performance statistics. Compared to Understat it covers many more aspects of the game by providing specific data (e.g. passes or defensive actions). However, it does have some issues: it does not provide a free API, the links do not have a fixed structure suitable for web scraping and the data is only available from the 2017/2018 season onwards. Despite this, we feel it is essential to obtain performance statistics in addition to those from Understat to improve our dataset. Therefore we decide to reduce the number of seasons that our dataset will be able to cover in order to increase the completeness of the data. The data are in a tabular format and available for download in csv format, but we decide to use web scraping to be able to select the data more easily and obtain json files immediately. To implement the web scraping we have two options: get a url provided by Fbref that sends directly to the table of interest, simplifying the code, or use the url of the page where the table of interest is located. We choose the second option because although it complicates the code, it is faster to copy the links to the pages of interest than to obtain the links to the tables. </p>\n",
        "\n",
        "<p>We take data on players passing and defensive actions. For both aspects of the game, we select the metrics that seem most meaningful to us from those available. Again, it is necessary to convert the values of some fields from strings to numerical values. We normalise the values in the Pos column and keep only the first value. We decide that a player can only have one role, because we think this is the best solution for the purpose of our dataset. We consider only the first role indicated.</p>\n",
        "\n",
        "<p>Using the same process as above with a few minor changes we ensure that the player names are the same as those provided by Understat. We rewrite the football_players list so that it starts from the 2017 season in order to use the same index matching mechanism. We modify the checkname function, which in this case receives one string less, and does not receive the full name of the player because Fbref does not provide it. It is not necessary to use the players_teams list in this case because the name formats are already quite similar.</p>\n",
        "\n",
        "<p>We generate a json file for each season and league, so we end up with 20 json files (5 leagues, 4 seasons).</p>"
      ],
      "metadata": {
        "id": "F4l_wRtzx4lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we make football_players starting from 2017/2018 season\n",
        "football_players = football_players[15:]\n",
        "players_teams = players_teams[15:]\n",
        "\n",
        "def checkname(players, name, squadre_understat, squadra_fbref):\n",
        "  maximum = 0\n",
        "\n",
        "  res = ''\n",
        "\n",
        "  if(len(name.split(' ')) > 1):\n",
        "    first_name = name.split(' ')[0]\n",
        "    last_name = name.split(' ')[1]\n",
        "\n",
        "    for i in range(len(players)):\n",
        "      massimo = max(SequenceMatcher(None, players[i], name).ratio(), SequenceMatcher(None, players[i], first_name).ratio(), SequenceMatcher(None, players[i], last_name).ratio())\n",
        "      if(maximum < massimo):\n",
        "        plus = 0\n",
        "        if(SequenceMatcher(None, squadre_understat[i], squadra_fbref).ratio() > 0.7):\n",
        "          plus = plus + 0.2\n",
        "\n",
        "        maximum = massimo\n",
        "        res = players[i]\n",
        "  else:\n",
        "    for i in range(len(players)):\n",
        "      if(maximum < SequenceMatcher(None, players[i], name).ratio()):\n",
        "        plus = 0\n",
        "        if(SequenceMatcher(None, squadre_understat[i], squadra_fbref).ratio() > 0.7):\n",
        "          plus = plus + 0.2\n",
        "\n",
        "        maximum = SequenceMatcher(None, players[i], name).ratio() + plus\n",
        "        res = players[i]\n",
        "\n",
        "  if(maximum > 0.8):\n",
        "    return res\n",
        "  else:\n",
        "    return ''"
      ],
      "metadata": {
        "id": "V0w4hhfx9295"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vthDUE5p9Ik",
        "outputId": "e276c36f-eb16-421e-c033-21fa25657142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "zBbdly8vqAQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-1gamcorX5Q",
        "outputId": "73a13a60-124e-4f3f-e93b-244871e1c4de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [76.0 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,823 kB]\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [872 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [934 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,516 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,463 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,954 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [738 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [771 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,242 kB]\n",
            "Get:26 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Fetched 14.7 MB in 4s (3,953 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 58 not upgraded.\n",
            "Need to get 95.3 MB of archives.\n",
            "After this operation, 327 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 97.0.4692.71-0ubuntu0.18.04.1 [1,142 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 97.0.4692.71-0ubuntu0.18.04.1 [84.7 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 97.0.4692.71-0ubuntu0.18.04.1 [4,370 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 97.0.4692.71-0ubuntu0.18.04.1 [5,055 kB]\n",
            "Fetched 95.3 MB in 3s (27.8 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_97.0.4692.71-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_97.0.4692.71-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_97.0.4692.71-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_97.0.4692.71-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "#url of interest\n",
        "defensive_links =['https://fbref.com/en/comps/9/1631/defense/2017-2018-Premier-League-Stats',\n",
        "                  'https://fbref.com/en/comps/11/1640/defense/2017-2018-Serie-A-Stats',\n",
        "                  'https://fbref.com/en/comps/12/1652/defense/2017-2018-La-Liga-Stats',\n",
        "                  'https://fbref.com/en/comps/20/1634/defense/2017-2018-Bundesliga-Stats',\n",
        "                  'https://fbref.com/en/comps/13/1632/defense/2017-2018-Ligue-1-Stats',\n",
        "                  'https://fbref.com/en/comps/9/1889/defense/2018-2019-Premier-League-Stats',\n",
        "                  'https://fbref.com/en/comps/11/1896/defense/2018-2019-Serie-A-Stats',\n",
        "                  'https://fbref.com/en/comps/12/1886/defense/2018-2019-La-Liga-Stats',\n",
        "                  'https://fbref.com/en/comps/20/2109/defense/2018-2019-Bundesliga-Stats',\n",
        "                  'https://fbref.com/en/comps/13/2104/defense/2018-2019-Ligue-1-Stats',\n",
        "                  'https://fbref.com/en/comps/9/3232/defense/2019-2020-Premier-League-Stats',\n",
        "                  'https://fbref.com/en/comps/11/3260/defense/2019-2020-Serie-A-Stats',\n",
        "                  'https://fbref.com/en/comps/12/3239/defense/2019-2020-La-Liga-Stats',\n",
        "                  'https://fbref.com/en/comps/20/3248/defense/2019-2020-Bundesliga-Stats',\n",
        "                  'https://fbref.com/en/comps/13/3243/defense/2019-2020-Ligue-1-Stats',\n",
        "                  'https://fbref.com/en/comps/9/10728/defense/2020-2021-Premier-League-Stats',\n",
        "                  'https://fbref.com/en/comps/11/10730/defense/2020-2021-Serie-A-Stats',\n",
        "                  'https://fbref.com/en/comps/12/10731/defense/2020-2021-La-Liga-Stats',\n",
        "                  'https://fbref.com/en/comps/20/10737/defense/2020-2021-Bundesliga-Stats',\n",
        "                  'https://fbref.com/en/comps/13/10732/defense/2020-2021-Ligue-1-Stats']\n",
        "\n",
        "iteration = 0\n",
        "for link in defensive_links:\n",
        "    page = driver.get(link)\n",
        "    time.sleep(2)\n",
        "\n",
        "    #we find the table\n",
        "    table = driver.find_element(By.XPATH , '//*[@id=\"stats_defense\"]')\n",
        "    table_html = table.get_attribute('innerHTML')\n",
        "    soup = BeautifulSoup(table_html, \"html.parser\")\n",
        "\n",
        "    #here are the headers columns of the dataframe where we will copy the table\n",
        "    headers = []\n",
        "    for i in soup.find_all('th'):\n",
        "        title = i.text\n",
        "        headers.append(title)\n",
        "    headers = headers[9:40]\n",
        "    df=pd.DataFrame(columns=headers)\n",
        "\n",
        "    #we build the dataset\n",
        "    for j in soup.find_all('tr')[2:]:\n",
        "        row_data = j.find_all('td')\n",
        "        row = [i.text for i in row_data]\n",
        "        if row:\n",
        "            length = len(df)\n",
        "            df.loc[length] = row\n",
        "\n",
        "    #we assaign the return value of checkname to the name field, we keep only one role\n",
        "    for i in range(len(df)-1):\n",
        "        name = df.loc[i, 'Player']\n",
        "        team = df.loc[i, 'Squad']\n",
        "        df.loc[i, 'Player'] = checkname(football_players[iteration], name, players_teams[iteration], team)\n",
        "        pos = df.loc[i, 'Pos']\n",
        "        if(len(pos.split(',')) > 1):\n",
        "            df.loc[i, 'Pos'] = pos.split(',')[0]\n",
        "\n",
        "    df = df[['Player', 'Pos', 'Tkl', 'TklW', 'Past', 'Press', 'Succ', 'Blocks', 'Int']]\n",
        "    df.columns = ['Player', 'Pos', 'Tkl', 'Tkl2', 'TklW', 'Past', 'Press', 'Succ', 'Blocks', 'Int']\n",
        "\n",
        "    for i in range(len(df) - 1):\n",
        "        if df.loc[i, 'Tkl']:\n",
        "            df.loc[i, 'Tkl'] = float(df.loc[i, 'Tkl'])\n",
        "        if df.loc[i, 'TklW']:\n",
        "            df.loc[i, 'TklW'] = float(df.loc[i, 'TklW'])\n",
        "        if df.loc[i, 'Past']:\n",
        "            df.loc[i, 'Past'] = float(df.loc[i, 'Past'])\n",
        "        if df.loc[i, 'Press']:\n",
        "            df.loc[i, 'Press'] = float(df.loc[i, 'Press'])\n",
        "        if df.loc[i, 'Succ'] :\n",
        "            df.loc[i, 'Succ'] = float(df.loc[i, 'Succ'])\n",
        "        if df.loc[i, 'Blocks']:\n",
        "            df.loc[i, 'Blocks'] = float(df.loc[i, 'Blocks'])\n",
        "        if df.loc[i, 'Int']:\n",
        "            df.loc[i, 'Int'] = float(df.loc[i, 'Int'])\n",
        "\n",
        "\n",
        "    iteration = iteration + 1\n",
        "\n",
        "    df.drop(df[df.Player == ''].index, inplace=True)\n",
        "    #we keep only the statistics that we are interested in\n",
        "    df = df[['Player', 'Pos', 'Tkl', 'TklW', 'Past', 'Press', 'Succ', 'Blocks', 'Int']]\n",
        "\n",
        "    link = link.split('/')[-1]\n",
        "    df = df.reset_index(drop=True)\n",
        "    df.to_json('defensive' + link + '.json', orient='records', force_ascii=False)\n",
        "\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "ywMTgR2G0dtS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "dce1cbbe-44e0-43ee-de38-6001d4098046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a7ef7bbbb240>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Player'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mteam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Squad'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Player'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfootball_players\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayers_teams_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-5d2282be5cdb>\u001b[0m in \u001b[0;36mcheckname\u001b[0;34m(players, name, squadre_understat, squadra_fbref)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaximum\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmassimo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mplus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequenceMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquadre_understat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquadra_fbref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m           \u001b[0mplus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplus\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#passing statistics\n",
        "driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "passing_links = ['https://fbref.com/en/comps/9/1631/passing/2017-2018-Premier-League-Stats',\n",
        "                  'https://fbref.com/en/comps/11/1640/passing/2017-2018-Serie-A-Stats',\n",
        "                  'https://fbref.com/en/comps/12/1652/passing/2017-2018-La-Liga-Stats',\n",
        "                  'https://fbref.com/en/comps/20/1634/passing/2017-2018-Bundesliga-Stats',\n",
        "                  'https://fbref.com/en/comps/13/1632/passing/2017-2018-Ligue-1-Stats',\n",
        "                  'https://fbref.com/en/comps/9/1889/passing/2018-2019-Premier-League-Stats',\n",
        "                  'https://fbref.com/en/comps/11/1896/passing/2018-2019-Serie-A-Stats',\n",
        "                  'https://fbref.com/en/comps/12/1886/passing/2018-2019-La-Liga-Stats',\n",
        "                  'https://fbref.com/en/comps/20/2109/passing/2018-2019-Bundesliga-Stats',\n",
        "                  'https://fbref.com/en/comps/13/2104/passing/2018-2019-Ligue-1-Stats',\n",
        "                  'https://fbref.com/en/comps/9/3232/passing/2019-2020-Premier-League-Stats',\n",
        "                  'https://fbref.com/en/comps/11/3260/passing/2019-2020-Serie-A-Stats',\n",
        "                  'https://fbref.com/en/comps/12/3239/passing/2019-2020-La-Liga-Stats',\n",
        "                  'https://fbref.com/en/comps/20/3248/passing/2019-2020-Bundesliga-Stats',\n",
        "                  'https://fbref.com/en/comps/13/3243/passing/2019-2020-Ligue-1-Stats',\n",
        "                  'https://fbref.com/en/comps/9/10728/passing/2020-2021-Premier-League-Stats',\n",
        "                  'https://fbref.com/en/comps/11/10730/passing/2020-2021-Serie-A-Stats',\n",
        "                  'https://fbref.com/en/comps/12/10731/passing/2020-2021-La-Liga-Stats',\n",
        "                  'https://fbref.com/en/comps/20/10737/passing/2020-2021-Bundesliga-Stats',\n",
        "                  'https://fbref.com/en/comps/13/10732/passing/2020-2021-Ligue-1-Stats']\n",
        "\n",
        "iteration = 0\n",
        "for link in passing_links:\n",
        "    page = driver.get(link)\n",
        "    time.sleep(2)\n",
        "\n",
        "    #we find the table\n",
        "    table = driver.find_element(By.XPATH , \"//*[@id='stats_passing']\")\n",
        "    table_html = table.get_attribute('innerHTML')\n",
        "    soup = BeautifulSoup(table_html, \"html.parser\")\n",
        "\n",
        "    #here are the headers columns of the dataframe where we will copy the table\n",
        "    headers = []\n",
        "    for i in soup.find_all('th'):\n",
        "        title = i.text\n",
        "        headers.append(title)\n",
        "    headers = headers[11:41]\n",
        "    df = pd.DataFrame(columns=headers)\n",
        "\n",
        "    #we build the dataframe\n",
        "    for j in soup.find_all('tr')[2:]:\n",
        "        row_data = j.find_all('td')\n",
        "        row = [i.text for i in row_data]\n",
        "        if row:\n",
        "            length = len(df)\n",
        "            df.loc[length] = row\n",
        "\n",
        "    #we assaign the return value of checkname to the name field, we keep only one role\n",
        "    for i in range(len(df)-1):\n",
        "        name = df.loc[i, 'Player']\n",
        "        team = df.loc[i, 'Squad']\n",
        "        df.loc[i, 'Player'] = checkname(football_players[iteration], name, players_teams_2[iteration], team)\n",
        "        pos = df.loc[i, 'Pos']\n",
        "        if (len(pos.split(',')) > 1):\n",
        "            df.loc[i, 'Pos'] = pos.split(',')[0]\n",
        "\n",
        "    df = df[['Player', 'Pos', 'Cmp', 'Cmp%', '1/3', 'PPA', 'CrsPA', 'Prog']]\n",
        "    df.columns = ['Player', 'Pos', 'Cmp', 'Cmp1', 'Cmp2', 'Cmp3', 'Cmp%', 'Cmp%1', 'Cmp%2', 'Cmp%3', '1/3', 'PPA',\n",
        "                  'CrsPA', 'Prog']\n",
        "\n",
        "    for i in range(len(df) - 1):\n",
        "        if df.loc[i, 'Cmp']:\n",
        "            df.loc[i, 'Cmp'] = float(df.loc[i, 'Cmp'])\n",
        "        if df.loc[i, 'Cmp%']:\n",
        "            df.loc[i, 'Cmp%'] = float(df.loc[i, 'Cmp%'])\n",
        "        if df.loc[i, '1/3']:\n",
        "            df.loc[i, '1/3'] = float(df.loc[i, '1/3'])\n",
        "        if df.loc[i, 'PPA']:\n",
        "            df.loc[i, 'PPA'] = float(df.loc[i, 'PPA'])\n",
        "        if df.loc[i, 'CrsPA'] :\n",
        "            df.loc[i, 'CrsPA'] = float(df.loc[i, 'CrsPA'])\n",
        "        if df.loc[i, 'Prog']:\n",
        "            df.loc[i, 'Prog'] = float(df.loc[i, 'Prog'])\n",
        "\n",
        "    iteration = iteration + 1\n",
        "    df.drop(df[df.Player == ''].index, inplace=True)\n",
        "    #we keep only the statistics that we are interested in\n",
        "    df = df[['Player', 'Pos', 'Cmp', 'Cmp%', '1/3', 'PPA', 'CrsPA', 'Prog']]\n",
        "\n",
        "    df.drop(df[df.Player == ''].index, inplace=True)\n",
        "    print(len(df))\n",
        "    link = link.split('/')[-1]\n",
        "    df = df.reset_index(drop=True)\n",
        "    df.to_json('passing' + link + '.json', orient = 'records', force_ascii=False)\n",
        "\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "eAvfalLe0ujj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e9c546-bba2-48ee-ebc5-2d0688158353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:69: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:71: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "310\n",
            "289\n",
            "303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data modelling\n",
        "<p>Once the data has been collected, we can determine the most suitable structure for our database. Given the structure of the downloaded data, the most suitable NoSQL model seems to be the document-based one. The logical structure is the following. We define a collection for each league of a certain season. The seasons range from 2017/2018 to 2020/2021, the leagues considered are the Premier League, Serie A, La Liga, Bundesliga and Ligue 1. Within each collection we have one document per player. Within each document there are the following fields: player name, age, nationality, height, weight, position, team. In addition, each document contains four other documents: general_stats containing the number of games played, total minutes played, yellow cards and red cards, offensive_stats containing statistics downloaded from Understat, passing_stats and defensive_stats containing statistics downloaded from Fbref. </p>\n",
        "\n"
      ],
      "metadata": {
        "id": "GjLf2NWXFXM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data aggregation\n",
        "<p>We use the name of the players as a reference field for the aggregation of the different datasets; with the procedure explained above we have ensured that this field is suitable for the purpose. Analysing the number of players in the files obtained, we observe that the files downloaded from api-football generally have fewer players. This is due to the greater difference in the name format (highlighted earlier) between Understat and api-football than the one between Understat and Fbref. We decide to start with the data downloaded from api-football and aggregate it with that obtained from Understat and Fbref.</p>"
      ],
      "metadata": {
        "id": "K73Ejl2ZETvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pymongo\n",
        "<p>We choose MongoDB as our document-based DBMS. To load and aggregate data we use pymongo, a python package for interacting with MongoDB databases.\n",
        "To aggregate the data we have more options. We can load all the json files obtained in a database creating a collection for each file and then use the option of aggregation between collections i.e. look up, as reference field the name of the players. The problem with this option is that the results of the aggregation are provided in the form of arrays, whereas we would rather to have the various game statistics as an Object type because it facilitates the navigation within the objects and because it is more logical, since all the  generated arrays would have only one element. Therefore we decide to load only the files obtained from api-football.com and then add the various statistics using the update_one function, which allows us to obtain Object types instead of Arrays. Using the input parameters of the function and the name of the players, we will carry out the aggregation.</p>.\n",
        "<p>We load the files with data from api-football.com so that we have a collection for each season of a certain league and an object for each player. For each season and league we load the contents of the corresponding file obtained from Understat into a list of python dictionaries. We load the statistics of the i-th dictionary of the list into the MongoDB object, whose value of the field indicating the name of the player is equal to that of the dictionary. We proceed in the same way with the files downloaded from Fbref. Here is the code. </p>"
      ],
      "metadata": {
        "id": "5XuBS-QyDlk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZGhBzFb1PgY",
        "outputId": "9a7fb7be-12db-425c-aee4-9f5913f0c372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (4.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# we add in the db the data downloaded from api-football.com\n",
        "myclient = MongoClient()\n",
        "db = myclient[\"Football_Players\"]\n",
        "\n",
        "seasons = ['2017','2018','2019','2020']\n",
        "leagues = ['epl','Serie A','La Liga','Bundesliga','Ligue 1']\n",
        "\n",
        "for season in seasons:\n",
        "    for league in leagues:\n",
        "        Collection = db[season+league]\n",
        "\n",
        "        with open(season + league + '_players.json', encoding='utf-8') as file:\n",
        "            file_data = json.load(file)\n",
        "\n",
        "        if isinstance(file_data, list):\n",
        "            Collection.insert_many(file_data)\n",
        "        else:\n",
        "            Collection.insert_one(file_data)\n",
        "\n",
        "for season in seasons:\n",
        "    for league in leagues:\n",
        "        with open('stats_'+ league + season + '_players.json', encoding='utf-8') as file:\n",
        "            file_data = json.load(file)\n",
        "\n",
        "        Collection = db[season + league]\n",
        "        for i in range(len(file_data)):\n",
        "            dizionario = file_data[i]\n",
        "            Collection.update_one({'name': dizionario['player_name']}, {'$set': {'team': dizionario['team_title']}})\n",
        "\n",
        "#we add the position (from Fbref)\n",
        "seasons_fbref = ['2017-2018','2018-2019','2019-2020','2020-2021']\n",
        "leagues_fbref = ['Premier-League','Serie-A','La-Liga','Bundesliga','Ligue-1']\n",
        "\n",
        "for season, season_fbref in zip(seasons, seasons_fbref):\n",
        "    for league, league_fbref in zip(leagues, leagues_fbref):\n",
        "        with open('defensive' + season_fbref + '-' + league_fbref + '-Stats.json', encoding='utf-8') as file:\n",
        "            file_data = json.load(file)\n",
        "\n",
        "        Collection = db[season+league]\n",
        "        for i in range(len(file_data)):\n",
        "            dizionario = file_data[i]\n",
        "            Collection.update_one({'name': dizionario['Player']}, {'$set': {'position': dizionario['Pos']}})\n",
        "\n",
        "#we add the general and offensive statistics\n",
        "for season in seasons:\n",
        "    for league in leagues:\n",
        "        with open('stats_'+ league + season + '_players.json', encoding='utf-8') as file:\n",
        "            file_data = json.load(file)\n",
        "\n",
        "        Collection = db[season + league]\n",
        "        for i in range(len(file_data)):\n",
        "            dizionario = file_data[i]\n",
        "            keys_to_extract = [\"games\", \"time\", \"red_cards\", \"yellow_cards\"]\n",
        "            dizionario_2 = {key: dizionario[key] for key in keys_to_extract}\n",
        "            Collection.update_one({'name': dizionario['player_name']}, {'$set': {'general_stats': dizionario_2}})\n",
        "            Collection.update_one({'name': dizionario['player_name']}, {'$set': {'offensive_stats': dizionario}})\n",
        "\n",
        "\n",
        "#we add the defensive statistics\n",
        "for season, season_fbref in zip(seasons, seasons_fbref):\n",
        "    for league, league_fbref in zip(leagues, leagues_fbref):\n",
        "        with open('defensive' + season_fbref + '-' + league_fbref + '-Stats.json', encoding='utf-8') as file:\n",
        "            file_data = json.load(file)\n",
        "\n",
        "        Collection = db[season+league]\n",
        "        for i in range(len(file_data)):\n",
        "            dizionario = file_data[i]\n",
        "            Collection.update_one({'name': dizionario['Player']}, {'$set': {'defensive_stats': dizionario}})\n",
        "\n",
        "#we add the passing statistics\n",
        "for season, season_fbref in zip(seasons, seasons_fbref):\n",
        "    for league, league_fbref in zip(leagues, leagues_fbref):\n",
        "        with open('passing' + season_fbref + '-' + league_fbref + '-Stats.json', encoding='utf-8') as file:\n",
        "            file_data = json.load(file)\n",
        "\n",
        "        Collection = db[season+league]\n",
        "        for i in range(len(file_data)):\n",
        "            dizionario = file_data[i]\n",
        "            Collection.update_one({'name': dizionario['Player']}, {'$set': {'passing_stats': dizionario}})"
      ],
      "metadata": {
        "id": "KkGTeFqtFZdj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "8f578209-fa3c-459e-f21d-517aee2bd18d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ServerSelectionTimeoutError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mServerSelectionTimeoutError\u001b[0m               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-f588a7620c1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mCollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mCollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymongo/collection.py\u001b[0m in \u001b[0;36minsert_many\u001b[0;34m(self, documents, ordered, bypass_document_validation, session)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mblk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Bulk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbypass_document_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_concern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mInsertManyResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minserted_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_concern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macknowledged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymongo/bulk.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, write_concern, session)\u001b[0m\n\u001b[1;32m    446\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_no_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_concern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymongo/bulk.py\u001b[0m in \u001b[0;36mexecute_command\u001b[0;34m(self, generator, write_concern, session)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tmp_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m             client._retry_with_session(\n\u001b[1;32m    345\u001b[0m                 self.is_retryable, retryable_bulk, s, self)\n",
            "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_tmp_session\u001b[0;34m(self, session, close)\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1616\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1617\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_ensure_session\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m   1601\u001b[0m             \u001b[0;31m# Don't make implicit sessions causally consistent. Applications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m             \u001b[0;31m# should always opt-in.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__start_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcausal_consistency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mConfigurationError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInvalidOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m             \u001b[0;31m# Sessions not supported.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m__start_session\u001b[0;34m(self, implicit, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__start_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimplicit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;31m# Raises ConfigurationError if sessions are not supported.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m         \u001b[0mserver_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_server_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSessionOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m         return client_session.ClientSession(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_get_server_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_server_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m         \u001b[0;34m\"\"\"Internal: start or resume a _ServerSession.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_topology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_server_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_return_server_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymongo/topology.py\u001b[0m in \u001b[0;36mget_server_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;31m# Sessions are always supported in load balanced mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_balanced\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m                 \u001b[0msession_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_session_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;31m# Sessions never time out in load balanced mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymongo/topology.py\u001b[0m in \u001b[0;36m_check_session_support\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m                     \u001b[0mreadable_server_selector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_selection_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                     None)\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0msession_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_description\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_session_timeout_minutes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymongo/topology.py\u001b[0m in \u001b[0;36m_select_servers_loop\u001b[0;34m(self, selector, timeout, address)\u001b[0m\n\u001b[1;32m    216\u001b[0m                 raise ServerSelectionTimeoutError(\n\u001b[1;32m    217\u001b[0m                     \u001b[0;34m\"%s, Timeout: %ss, Topology Description: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                     (self._error_message(selector), timeout, self.description))\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_opened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mServerSelectionTimeoutError\u001b[0m: localhost:27017: [Errno 111] Connection refused, Timeout: 30s, Topology Description: <TopologyDescription id: 61ed7842ec1d931957741761, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused')>]>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must now delete some data in order to arrive at the desired structure. We delete the id, injured and photo fields because we are not interested in. We delete the fields id, player_name, games, time, yellow_cards, red_cards, position and team_title from offensive_stats, because we have placed them in general_stats. We delete the fields Player and Pos from passing_stats and defensive_stats, because we have inserted them as general fields. Now we have the desired database."
      ],
      "metadata": {
        "id": "Dz56bnjaIA2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for season in seasons:\n",
        "    for league in leagues:\n",
        "        Collection = db[season + league]\n",
        "        Collection.update_many({}, {'$unset': {'id': '', 'injured': '', 'photo': '', 'offensive_stats.id': '',\n",
        "                                               'offensive_stats.player_name': '', 'offensive_stats.games': '',\n",
        "                                               'offensive_stas.time': '', 'offensive_stats.yellow_cards': '',\n",
        "                                               'offensive_stats.red_cards': '', 'offensive_stats.position': '',\n",
        "                                               'offensive_stas.team_title': '', 'passing_stats.Player': '',\n",
        "                                               'passing_stats.Pos': '', 'defensive_stats.Player': '',\n",
        "                                               'defensive_stats.Pos': ''}})"
      ],
      "metadata": {
        "id": "97jBQDu5I-kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seasons = ['2017', '2018','2019','2020']\n",
        "leagues = ['epl','Serie A','La Liga', 'Ligue 1']\n",
        "leagues_names=['PremierLeague', 'SerieA', 'LaLiga', 'Ligue1']\n",
        "\n",
        "for season in seasons:\n",
        "    for league, name in zip(leagues, leagues_names):\n",
        "        Collection = db[season+league]\n",
        "        Collection.rename(season+name)"
      ],
      "metadata": {
        "id": "oIu7_R-GyPq-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}